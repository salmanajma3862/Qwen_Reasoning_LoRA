{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhqLV815J6iM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# 1. Install Unsloth for 2x faster training and 70% less VRAM usage.\n",
        "# 2. We install xformers for efficient attention mechanisms.\n",
        "!pip install unsloth\n",
        "!pip install --no-deps \"xformers<0.0.29\" \"trl<0.13.0\" peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096 # Adjust based on how long you want the \"thinking\" to be\n",
        "dtype = None # None for auto detection\n",
        "load_in_4bit = True # Use 4-bit quantization to save memory\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-3B-Instruct\", # Excellent base for reasoning\n",
        "    max_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Apply LoRA (Low-Rank Adaptation)\n",
        "# This adds a small number of trainable weights to the model while keeping\n",
        "# the base weights frozen, allowing us to train on a consumer GPU.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Rank: Higher = more capacity but more VRAM. 16 is a sweet spot.\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Optimized to 0 for Unsloth\n",
        "    bias = \"none\",    # Optimized to \"none\" for Unsloth\n",
        "    use_gradient_checkpointing = \"unsloth\", # Saves massive VRAM\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "DhUPOtufNEfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# We use a reasoning dataset. 'Magpie-Reasoning' or 'OpenThoughts' are great.\n",
        "# For this example, we'll use a subset that has clear step-by-step logic.\n",
        "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split = \"train[:500]\")\n",
        "\n",
        "# Print the dataset features to identify correct column names\n",
        "print(dataset.features)\n",
        "\n",
        "# The Chat Template for Qwen (Imperfect/ChatML style)\n",
        "# We structure the 'assistant' response to ALWAYS start with <think>\n",
        "prompt_style = \"\"\"<|im_start|>system\n",
        "You are a helpful assistant that thinks step-by-step before answering.<|im_end|>\n",
        "<|im_start|>user\n",
        "{}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}<|im_end|>\"\"\"\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    # These keys need to be updated based on the actual dataset features\n",
        "    instructions = examples[\"Question\"]\n",
        "    reasoning    = examples[\"Complex_CoT\"] # The logic chain\n",
        "    responses    = examples[\"Response\"]    # The final answer\n",
        "    texts = []\n",
        "    for instruction, reason, response in zip(instructions, reasoning, responses):\n",
        "        # We combine them into the template\n",
        "        text = prompt_style.format(instruction, reason, response)\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "id": "mn2FIYy7QKGF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Total batch size = 8\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60, # Small number for testing; increase to 300+ for better results\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iohlpSAlTSmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable native 2x faster inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt_style.format(\n",
        "        \"\"\"Alice knows that Bob knows that Carol is wrong.\n",
        "Carol knows that Bob does not know whether Alice is lying.\n",
        "Can Alice know that Carol knows the truth? Why or why not?\"\"\", # Instruction\n",
        "        \"\", # Leave reasoning empty for generation\n",
        "        \"\"  # Leave response empty for generation\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "# We use a streamer to see the \"thinking\" happen in real time\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 512)"
      ],
      "metadata": {
        "id": "e-O43CWOYF8B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}